{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq7u0kdRwDje"
   },
   "source": [
    "# Fitting a simple Reinforcement Learning model to behavioral data with PyMC\n",
    "\n",
    "Reinforcement Learning models are commonly used in behavioral research to model how animals and humans learn, in situtions where they get to make repeated choices that are followed by some form of feedback, such as a reward or a punishment.\n",
    "\n",
    "In this notebook we will consider the simplest learning scenario, where there are only two possible actions. When an action is taken, it is always followed by an immediate reward. Finally, the outcome of each action is independent from the previous actions taken. This scenario is sometimes referred to as the [multi-armed bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit).\n",
    "\n",
    "\n",
    "Let's say that the two actions (e.g., left and right buttons) are associated with a unit reward 40% and 60% of the time, respectively. At the beginning the learning agent does not know which action $a$ is better, so they may start by assuming both actions have a mean value of 50%. We can store these values in a table, which is usually referred to as a $Q$ table:\n",
    "\n",
    "$$ Q = \\begin{cases}\n",
    "      .5, a = \\text{left}\\\\\n",
    "      .5, a = \\text{right}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "When an action is chosen and a reward $r = \\{0,1\\}$ is observed, the estimated value of that action is updated as follows:\n",
    "\n",
    "$$Q_{a} = Q_{a} + \\alpha (r - Q_{a})$$\n",
    "\n",
    "where $\\alpha \\in [0, 1]$ is a learning parameter that influences how much the value of an action is shifted towards the observed reward in each trial. Finally, the $Q$ table values are converted into action probabilities via the softmax transformation:\n",
    "\n",
    "$$ P(a = \\text{right}) = \\frac{\\exp(\\beta Q_{\\text{right}})}{\\exp(\\beta Q_{\\text{right}}) + \\exp(\\beta Q_{\\text{left}})}$$\n",
    "\n",
    "where the $\\beta \\in (0, +\\infty)$ parameter determines the level of noise in the agent choices. Larger values will be associated with more deterministic choices and smaller values with increasingly random choices.\n",
    "\n",
    "***\n",
    "\n",
    "### Credits\n",
    "* The PyMC code was adapted from that of Maria Eckstein ([GitHub](https://github.com/MariaEckstein/SLCN), [PyMC Discourse](https://discourse.pymc.io/t/modeling-reinforcement-learning-of-human-participant-using-pymc3/1735))\n",
    "* The MLE code was adapted from that of Robert Wilson and Anne Collins ([Ten simple rules for the computational modeling of behavioral data](https://elifesciences.org/articles/49547), [GitHub](https://github.com/AnneCollins/TenSimpleRulesModeling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QTq-0HMw7dBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "The installed Theano(-PyMC) version (1.0.5) does not match the PyMC3 requirements.\n",
      "It was imported from ['/home/petr/anaconda3/envs/robostackenv/lib/python3.8/site-packages/theano']\n",
      "For PyMC3 to work, a compatible Theano-PyMC backend version must be installed.\n",
      "See https://github.com/pymc-devs/pymc3/wiki for installation instructions.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TheanoConfigParser' object has no attribute 'gcc__cxxflags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maesara\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maesara\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mat\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/robostackenv/lib/python3.8/site-packages/pymc3/__init__.py:79\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m _check_backend_version()\n\u001b[0;32m---> 79\u001b[0m \u001b[43m__set_compiler_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m _hotfix_theano_printing()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymc3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gp, ode, sampling\n",
      "File \u001b[0;32m~/anaconda3/envs/robostackenv/lib/python3.8/site-packages/pymc3/__init__.py:61\u001b[0m, in \u001b[0;36m__set_compiler_flags\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set_compiler_flags\u001b[39m():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Workarounds for Theano compiler problems on various platforms\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[43mtheano\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcc__cxxflags\u001b[49m\n\u001b[1;32m     62\u001b[0m     theano\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgcc__cxxflags \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -Wno-c++11-narrowing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TheanoConfigParser' object has no attribute 'gcc__cxxflags'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import pymc3 as pm\n",
    "import aesara\n",
    "import aesara.tensor as at\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuX-31cMC3u7"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "seed = sum(map(ord, \"RL_PyMC\"))\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BmQhhxi6Ol5"
   },
   "outputs": [],
   "source": [
    "def plot_data(actions, rewards, Qs):\n",
    "    plt.figure(figsize=(20,3))\n",
    "    x = np.arange(len(actions))\n",
    "\n",
    "    plt.plot(x, Qs[:, 0] - .5 + 0, c='C0', lw=3, alpha=.3)\n",
    "    plt.plot(x, Qs[:, 1] - .5 + 1, c='C1', lw=3, alpha=.3)\n",
    "\n",
    "    s = 50\n",
    "    lw = 2\n",
    "\n",
    "    cond = (actions == 0) & (rewards == 0)\n",
    "    plt.scatter(x[cond], actions[cond], s=s, c='None', ec='C0', lw=lw)\n",
    "\n",
    "    cond = (actions == 0) & (rewards == 1)\n",
    "    plt.scatter(x[cond], actions[cond], s=s, c='C0', ec='C0', lw=lw)\n",
    "\n",
    "    cond = (actions == 1) & (rewards == 0)\n",
    "    plt.scatter(x[cond], actions[cond], s=s, c='None', ec='C1', lw=lw)\n",
    "\n",
    "    cond = (actions == 1) & (rewards == 1)\n",
    "    plt.scatter(x[cond], actions[cond], s=s, c='C1', ec='C1', lw=lw)\n",
    "\n",
    "    plt.scatter(0, 20, c='k', s=s, lw=lw, label='Reward')\n",
    "    plt.scatter(0, 20, c='w', ec='k', s=s, lw=lw, label='No reward')\n",
    "    plt.plot([0,1], [20, 20], c='k', lw=3, alpha=.3, label='Qvalue (centered)')\n",
    "\n",
    "\n",
    "    plt.yticks([0,1], ['left', 'right'])\n",
    "    plt.ylim(-1, 2)\n",
    "\n",
    "    plt.ylabel('action')\n",
    "    plt.xlabel('trial')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = (1,2,0)\n",
    "    handles = [handles[idx] for idx in order]\n",
    "    labels = [labels[idx] for idx in order]\n",
    "\n",
    "    plt.legend(handles, labels, fontsize=12, loc=(1.01, .27))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG_Nxvr5wC4B"
   },
   "source": [
    "## Generating fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcPVL7kZ8Zs2"
   },
   "outputs": [],
   "source": [
    "def generate_data(rng, alpha, beta, n=100, p_r=[.4, .6]):\n",
    "    actions = np.zeros(n, dtype=\"int\")\n",
    "    rewards = np.zeros(n, dtype=\"int\")\n",
    "    Qs = np.zeros((n, 2))\n",
    "\n",
    "    # Initialize Q table\n",
    "    Q = np.array([.5, .5])\n",
    "    for i in range(n):\n",
    "        # Apply the Softmax transformation\n",
    "        exp_Q = np.exp(beta*Q)\n",
    "        prob_a = exp_Q / np.sum(exp_Q)\n",
    "\n",
    "        # Simulate choice and reward\n",
    "        a = rng.choice([0, 1], p=prob_a)\n",
    "        r = rng.random() < p_r[a]\n",
    "\n",
    "        # Update Q table\n",
    "        Q[a] = Q[a] + alpha * (r - Q[a])\n",
    "\n",
    "        # Store values\n",
    "        actions[i] = a\n",
    "        rewards[i] = r\n",
    "        Qs[i] = Q.copy()\n",
    "\n",
    "    return actions, rewards, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceNagbmsZXW6"
   },
   "outputs": [],
   "source": [
    "true_alpha = .5\n",
    "true_beta = 5\n",
    "n = 150\n",
    "actions, rewards, Qs = generate_data(rng, true_alpha, true_beta, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "MDhJI8vOXZeU",
    "outputId": "60f7ee37-2d1f-44ad-afff-b9ba7d82a8d8"
   },
   "outputs": [],
   "source": [
    "plot_data(actions, rewards, Qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RNLAtqDXgG_"
   },
   "source": [
    "The plot above shows a simulated run of 150 trials, with parameters $\\alpha = .5$ and $\\beta = 5$, and constant reward probabilities of $.4$ and $.6$ for the left (blue) and right (orange) actions, respectively. \n",
    "\n",
    "Solid and empty dots indicate actions followed by rewards and no-rewards, respectively. The solid line shows the estimated $Q$ value for each action centered around the respective colored dots (the line is above its dots when the respective $Q$ value is above $.5$, and below otherwise). It can be seen that this value increases with rewards (solid dots) and decreases with non-rewards (empty dots). \n",
    "\n",
    "The change in line height following each outcome is directly related to the $\\alpha$ pamater. The influence of the $\\beta$ parameter is more difficult to grasp, but one way to think about it is that the higher its value, the higher the probability of flipping between actions regardless of their estimated values. \n",
    "\n",
    "We can also see that the value of the unchosen action is not altered by the outcome of the chosen action. This is a strong assumption of this model, which could be tested by comparing it to an alternative model where this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUTfha8Hc1ap"
   },
   "source": [
    "## Estimating the learning parameters via Maximum Likelihood\n",
    "\n",
    "Having generated the data, the goal is to now 'invert the model' to estimate the learning parameters $\\alpha$ and $\\beta$. I start by doing it via Maximum Likelihood Estimation (MLE). This requires writing a custom function that computes the likelihood of the data given a potential $\\alpha$ and $\\beta$ and the fixed observed actions and rewards (actually the function computes the negative log likelihood, in order to avoid underflow issues).\n",
    "\n",
    "I employ the handy scipy.optimize.minimize function, to quickly retrieve the values of $\\alpha$ and $\\beta$ that maximize the likelihood of the data (or actually, minimize the negative log likelihood).\n",
    "\n",
    "This was also helpful when I later wrote the Aesara function that computed the choice probabilities in PyMC. First, the underlying logic is the same, the only thing that changes is the syntax. Second, it provides a way to be confident that I did not mess up, and what I was actually computing was what I intended to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWGlRE3BjR0E"
   },
   "outputs": [],
   "source": [
    "def llik_td(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    actions, rewards = args\n",
    "\n",
    "    # Initialize values\n",
    "    Q = np.array([.5, .5])\n",
    "    logp_actions = np.zeros(len(actions))\n",
    "\n",
    "    for t, (a, r) in enumerate(zip(actions,rewards)):\n",
    "        # Apply the softmax transformation\n",
    "        Q_ = Q * beta\n",
    "        logp_action = Q_ - scipy.special.logsumexp(Q_)\n",
    "\n",
    "        # Store the log probability of the observed action\n",
    "        logp_actions[t] = logp_action[a]\n",
    "\n",
    "        # Update the Q values for the next trial\n",
    "        Q[a] = Q[a] + alpha * (r - Q[a])\n",
    "\n",
    "    # Return the negative log likelihood of all observed actions\n",
    "    return -np.sum(logp_actions[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXZgywFIgz6J"
   },
   "source": [
    "The function `llik_td` is strikingly similar to the `generate_data` one, except that instead of simulating an action and reward in each trial, it stores the log-probability of the observed action.\n",
    "\n",
    "The function `scipy.special.logsumexp` is used to compute the term $\\log(\\exp(\\beta Q_{\\text{right}}) + \\exp(\\beta Q_{\\text{left}}))$ in a way that is more numerically stable. \n",
    "\n",
    "In the end, the function returns the negative sum of all the log probabilities, which is equivalent to multiplying the probabilities in their original scale.\n",
    "\n",
    "(The first action is ignored just to make the output comparable to the later Aesara function. It doesn't actually change any estimation, as the initial probabilities are fixed and do not depend on either the $\\alpha$ or $\\beta$ parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-E8B-rrBgy0j",
    "outputId": "7c18b426-8d50-4706-f940-45ec716877f4"
   },
   "outputs": [],
   "source": [
    "llik_td([true_alpha, true_beta], *(actions, rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT2UwuKWvRCq"
   },
   "source": [
    "Above, I computed the negative log likelihood of the data given the true $\\alpha$ and $\\beta$ parameters.\n",
    "\n",
    "Below, I let scipy find the MLE values for the two parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "W1MOBxvw4Zl9",
    "outputId": "39a73f7a-2362-4ef7-cc03-1e9aeda35ecf"
   },
   "outputs": [],
   "source": [
    "x0 = [true_alpha, true_beta]\n",
    "result = scipy.optimize.minimize(llik_td, x0, args=(actions, rewards), method='BFGS')\n",
    "print(result)\n",
    "print('')\n",
    "print(f'MLE: alpha = {result.x[0]:.2f} (true value = {true_alpha})')\n",
    "print(f'MLE: beta = {result.x[1]:.2f} (true value = {true_beta})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cXP93QeVVM"
   },
   "source": [
    "The estimated MLE values are relatively close to the true ones. However, this procedure does not give any idea of the plausible uncertainty around these parameter values. To get that, I'll turn to PyMC for a bayesian posterior estimation.\n",
    "\n",
    "But before that, I will implement a simple vectorization optimization to the log-likelihood function that will be more similar to the Aesara counterpart. The reason for this is to speed up the slow bayesian inference engine down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4knb5sKW9V66"
   },
   "outputs": [],
   "source": [
    "def llik_td_vectorized(x, *args):\n",
    "    # Extract the arguments as they are passed by scipy.optimize.minimize\n",
    "    alpha, beta = x\n",
    "    actions, rewards = args\n",
    "\n",
    "    # Create a list with the Q values of each trial\n",
    "    Qs = np.ones((n, 2), dtype=\"float64\")\n",
    "    Qs[0] = .5\n",
    "    for t, (a, r) in enumerate(zip(actions[:-1], rewards[:-1])):  # The last Q values were never used, so there is no need to compute them\n",
    "        Qs[t+1, a] = Qs[t, a] + alpha * (r - Qs[t, a])\n",
    "        Qs[t+1, 1-a] = Qs[t, 1-a]\n",
    "\n",
    "    # Apply the softmax transformation in a vectorized way\n",
    "    Qs_ = Qs * beta\n",
    "    logp_actions = Qs_ - scipy.special.logsumexp(Qs_, axis=1)[:, None]\n",
    "\n",
    "    # Return the logp_actions for the observed actions\n",
    "    logp_actions = logp_actions[np.arange(len(actions)), actions]\n",
    "    return -np.sum(logp_actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w9Z_Ik7AlBQC",
    "outputId": "445a7838-29d0-4f21-bfd8-5b65606af286"
   },
   "outputs": [],
   "source": [
    "llik_td_vectorized([true_alpha, true_beta], *(actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bDPZJe7RqCZX",
    "outputId": "a90fbb47-ee9b-4390-87ff-f4b39ece8fca"
   },
   "outputs": [],
   "source": [
    "%timeit llik_td([true_alpha, true_beta], *(actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Dvrqf878swBX",
    "outputId": "94bf3268-0eab-4ce9-deb9-5d1527b3c19d"
   },
   "outputs": [],
   "source": [
    "%timeit llik_td_vectorized([true_alpha, true_beta], *(actions, rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAs_zpPZyopT"
   },
   "source": [
    "The vectorized function gives the same results, but runs almost one order of magnitude faster. \n",
    "\n",
    "When implemented as an Aesara function, the difference between the vectorized and standard versions was not this drastic. Still, it ran twice as fast, which meant the model also sampled at twice the speed it would otherwise have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC7xbCCIL7K4"
   },
   "source": [
    "\n",
    "## Estimating the learning parameters via PyMC\n",
    "\n",
    "The most challenging part was to create an Aesara function/loop to estimate the Q values when sampling our parameters with PyMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8L_FAB4hle1"
   },
   "outputs": [],
   "source": [
    "def update_Q(action, reward,\n",
    "             Qs,\n",
    "             alpha):\n",
    "    \"\"\"\n",
    "    This function updates the Q table according to the RL update rule.\n",
    "    It will be called by aesara.scan to do so recursevely, given the observed data and the alpha parameter\n",
    "    This could have been replaced be the following lamba expression in the aesara.scan fn argument:\n",
    "        fn=lamba action, reward, Qs, alpha: at.set_subtensor(Qs[action], Qs[action] + alpha * (reward - Qs[action]))\n",
    "    \"\"\"\n",
    "\n",
    "    Qs = at.set_subtensor(Qs[action], Qs[action] + alpha * (reward - Qs[action]))\n",
    "    return Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHzhTy20g4vh"
   },
   "outputs": [],
   "source": [
    "# Transform the variables into appropriate Aesara objects\n",
    "rewards_ = at.as_tensor_variable(rewards, dtype='int32')\n",
    "actions_ = at.as_tensor_variable(actions, dtype='int32')\n",
    "\n",
    "alpha = at.scalar(\"alpha\")\n",
    "beta = at.scalar(\"beta\")\n",
    "\n",
    "# Initialize the Q table\n",
    "Qs = 0.5 * at.ones((2,), dtype='float64')\n",
    "\n",
    "# Compute the Q values for each trial\n",
    "Qs, _ = aesara.scan(\n",
    "    fn=update_Q,\n",
    "    sequences=[actions_, rewards_],\n",
    "    outputs_info=[Qs],\n",
    "    non_sequences=[alpha])\n",
    "\n",
    "# Apply the softmax transformation\n",
    "Qs = Qs * beta\n",
    "logp_actions = Qs - at.logsumexp(Qs, axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the negative log likelihod of the observed actions\n",
    "logp_actions = logp_actions[at.arange(actions_.shape[0]-1), actions_[1:]]\n",
    "neg_loglike = -at.sum(logp_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9Ayn6-kzhPN"
   },
   "source": [
    "Let's wrap it up in a function to test out if it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "g1hkTd75xxwo",
    "outputId": "a2310fd3-cac2-48c6-9d22-3c3b72410427"
   },
   "outputs": [],
   "source": [
    "aesara_llik_td = aesara.function(inputs=[alpha, beta], outputs=neg_loglike, on_unused_input=\"ignore\")\n",
    "result = aesara_llik_td(true_alpha, true_beta)\n",
    "float(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmcoU1CF5ix-"
   },
   "source": [
    "The same result is obtained, so we can be confident that the Aesara loop is working as expected. We are now ready to implement the PyMC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c70L4ZBT7QLr"
   },
   "outputs": [],
   "source": [
    "def aesara_llik_td(alpha, beta, actions, rewards):\n",
    "    rewards = at.as_tensor_variable(rewards, dtype='int32')\n",
    "    actions = at.as_tensor_variable(actions, dtype='int32')\n",
    "\n",
    "    # Compute the Qs values\n",
    "    Qs = 0.5 * at.ones((2,), dtype='float64')\n",
    "    Qs, updates = aesara.scan(\n",
    "        fn=update_Q,\n",
    "        sequences=[actions, rewards],\n",
    "        outputs_info=[Qs],\n",
    "        non_sequences=[alpha])\n",
    "\n",
    "    # Apply the sotfmax transformation\n",
    "    Qs = Qs[:-1] * beta\n",
    "    logp_actions = Qs - at.logsumexp(Qs, axis=1, keepdims=True)\n",
    "\n",
    "    # Calculate the log likelihood of the observed actions\n",
    "    logp_actions = logp_actions[at.arange(actions.shape[0]-1), actions[1:]]\n",
    "    return at.sum(logp_actions)  # PyMC expects the standard log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "XQNBZLMvAdbo",
    "outputId": "65d7a861-476c-4598-985c-e0b0fcd744c4"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m:\n",
    "    alpha = pm.Beta('alpha', 1, 1)\n",
    "    beta = pm.HalfNormal('beta', 10)\n",
    "\n",
    "    like = pm.Potential('like', aesara_llik_td(alpha, beta, actions, rewards))\n",
    "\n",
    "    tr = pm.sample(random_seed=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "vgSumt-oATfN",
    "outputId": "eb3348a4-3092-48c8-d8b4-678af0173079"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(tr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "BL84iT_RAzEL",
    "outputId": "dcd4174b-4148-45cb-f72d-973f1487d8c2"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(tr, ref_val=[true_alpha, true_beta]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FtAp76PBLCr"
   },
   "source": [
    "In this example, the obtained posteriors are nicely centered around the MLE values. What we have gained is an idea of the plausible uncertainty around these values.\n",
    "\n",
    "### Bonus: Alternative model using Bernoulli for the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQdszDk_qYCX"
   },
   "outputs": [],
   "source": [
    "def right_action_probs(alpha, beta, actions, rewards):\n",
    "    rewards = at.as_tensor_variable(rewards, dtype='int32')\n",
    "    actions = at.as_tensor_variable(actions, dtype='int32')\n",
    "\n",
    "    # Compute the Qs values\n",
    "    Qs = 0.5 * at.ones((2,), dtype='float64')\n",
    "    Qs, updates = aesara.scan(\n",
    "        fn=update_Q,\n",
    "        sequences=[actions, rewards],\n",
    "        outputs_info=[Qs],\n",
    "        non_sequences=[alpha])\n",
    "\n",
    "    # Apply the sotfmax transformation\n",
    "    Qs = Qs[:-1] * beta\n",
    "    logp_actions = Qs - at.logsumexp(Qs, axis=1, keepdims=True)\n",
    "\n",
    "    # Return the probabilities for the right action, in the original scale\n",
    "    return at.exp(logp_actions[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "S55HgqZiTfpa",
    "outputId": "a2db2d68-8bf3-4773-8368-5b6dff310e4b"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_alt:\n",
    "    alpha = pm.Beta('alpha', 1, 1)\n",
    "    beta = pm.HalfNormal('beta', 10)\n",
    "\n",
    "    action_probs = right_action_probs(alpha, beta, actions, rewards)\n",
    "    like = pm.Bernoulli('like', p=action_probs, observed=actions[1:])\n",
    "\n",
    "    tr_alt = pm.sample(random_seed=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "zjXW103JiDRQ",
    "outputId": "aafc1b1e-082e-414b-cac7-0ad805097057"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(tr_alt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDJN2w117eox"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(tr_alt, ref_val=[true_alpha, true_beta]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrXjM5BT9U0CqhtRb/rE8X",
   "include_colab_link": true,
   "name": "RL PyMC.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "robostackenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
